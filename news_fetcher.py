import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import logging
import json
import os
from typing import List, Dict, Optional
from config import NEWS_SOURCES

logger = logging.getLogger(__name__)

class NewsItem:
    def __init__(self, title: str, content: str, url: str, source: str, 
                 published_date: datetime, summary: str = ""):
        self.title = title
        self.content = content
        self.url = url
        self.source = source
        self.published_date = published_date
        self.summary = summary
        
        # Daha gÃ¼vÉ™nli hash mexanizmi
        # URL vÉ™ baÅŸlÄ±ÄŸÄ± normalize et
        normalized_url = url.split('?')[0].strip().lower()  # Query parametrlÉ™ri sil
        normalized_title = ''.join(title.strip().lower().split())  # BoÅŸluqlarÄ± sil
        
        # Hash yaratmaq Ã¼Ã§Ã¼n normalize edilmiÅŸ mÉ™lumatlarÄ± istifadÉ™ et
        hash_string = f"{normalized_title}{normalized_url}{source.lower()}"
        self.hash = abs(hash(hash_string))

    def __eq__(self, other):
        return isinstance(other, NewsItem) and self.hash == other.hash

    def __hash__(self):
        return self.hash
    
    def to_dict(self):
        """XÉ™bÉ™ri dictionary formatÄ±na Ã§evirir"""
        return {
            'title': self.title,
            'url': self.url,
            'source': self.source,
            'published_date': self.published_date.isoformat(),
            'hash': self.hash
        }

class NewsFetcher:
    def __init__(self):
        self.seen_news_file = 'seen_news.json'
        self.seen_news = set()
        self._load_seen_news()

    def _load_seen_news(self):
        """ÆvvÉ™lcÉ™dÉ™n gÃ¶rÃ¼lÉ™n xÉ™bÉ™rlÉ™ri fayldan yÃ¼klÉ™yir"""
        try:
            if os.path.exists(self.seen_news_file):
                with open(self.seen_news_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # Daha sÉ™rt tarix filteri - yalnÄ±z son 6 saat É™rzindÉ™ki xÉ™bÉ™rlÉ™r
                current_time = datetime.now()
                cutoff_time = current_time - timedelta(hours=6)  # 24-dÉ™n 6 saata endirildi
                
                valid_items = []
                for item in data:
                    try:
                        # Tarix yoxlamasÄ± daha dÉ™qiq
                        saved_at_str = item.get('saved_at')
                        if not saved_at_str:
                            # KÃ¶hnÉ™ formatda tarix yoxdursa, published_date istifadÉ™ et
                            saved_at_str = item.get('published_date')
                        
                        if saved_at_str:
                            saved_at = datetime.fromisoformat(saved_at_str)
                            # YalnÄ±z son 6 saat É™rzindÉ™ki xÉ™bÉ™rlÉ™ri saxla
                            if saved_at > cutoff_time:
                                self.seen_news.add(item['hash'])
                                valid_items.append(item)
                        else:
                            # Tarix mÉ™lumatÄ± olmayan kÃ¶hnÉ™ xÉ™bÉ™rlÉ™ri atla
                            logger.debug(f"Tarix mÉ™lumatÄ± olmayan xÉ™bÉ™r atlanÄ±ldÄ±: {item.get('title', 'N/A')[:50]}")
                            
                    except Exception as e:
                        logger.warning(f"XÉ™bÉ™r item yÃ¼klÉ™nmÉ™ xÉ™tasÄ±: {e}")
                        continue
                
                # Fayl yenilÉ™nmiÅŸ mÉ™lumatlarla saxla (kÃ¶hnÉ™lÉ™ri sil)
                if len(valid_items) < len(data):
                    try:
                        with open(self.seen_news_file, 'w', encoding='utf-8') as f:
                            json.dump(valid_items, f, ensure_ascii=False, indent=2)
                        logger.info(f"KÃ¶hnÉ™ {len(data) - len(valid_items)} xÉ™bÉ™r fayldan silindi")
                    except Exception as e:
                        logger.error(f"Fayl yenilÉ™nmÉ™ xÉ™tasÄ±: {e}")
                
                logger.info(f"{len(self.seen_news)} yeni xÉ™bÉ™r hash-i yÃ¼klÉ™ndi (son 6 saat)")
            else:
                logger.info("seen_news.json faylÄ± tapÄ±lmadÄ±, yeni yaradÄ±lacaq")
                self.seen_news = set()
                
        except Exception as e:
            logger.error(f"GÃ¶rÃ¼lÉ™n xÉ™bÉ™rlÉ™r yÃ¼klÉ™nmÉ™ xÉ™tasÄ±: {e}")
            # Problem olduqda, tÉ™miz baÅŸla
            self.seen_news = set()
            if os.path.exists(self.seen_news_file):
                try:
                    # Korrupted faylÄ± backup kimi saxla
                    backup_file = f"{self.seen_news_file}.backup"
                    os.rename(self.seen_news_file, backup_file)
                    logger.info(f"Korrupted fayl {backup_file} olaraq backup edildi")
                except Exception:
                    pass

    def _save_seen_news(self, news_item: NewsItem = None):
        """GÃ¶rÃ¼lÉ™n xÉ™bÉ™rlÉ™ri fayla saxlayÄ±r"""
        try:
            # MÃ¶vcud mÉ™lumatlarÄ± yÃ¼klÉ™yir
            existing_data = []
            if os.path.exists(self.seen_news_file):
                try:
                    with open(self.seen_news_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                except:
                    existing_data = []
            
            # Yeni xÉ™bÉ™r É™lavÉ™ edir
            if news_item:
                new_entry = {
                    'hash': news_item.hash,
                    'title': news_item.title[:100],  # Ä°lk 100 simvol
                    'source': news_item.source,
                    'url': news_item.url,
                    'published_date': news_item.published_date.isoformat(),
                    'saved_at': datetime.now().isoformat()
                }
                
                # DublikatlarÄ± yoxlayÄ±r
                if not any(item['hash'] == news_item.hash for item in existing_data):
                    existing_data.append(new_entry)
            
            # Son 24 saat É™rzindÉ™ki xÉ™bÉ™rlÉ™ri filtr edir
            current_time = datetime.now()
            cutoff_time = current_time - timedelta(hours=24)
            
            filtered_data = []
            for item in existing_data:
                try:
                    saved_at = datetime.fromisoformat(item['saved_at'])
                    if saved_at > cutoff_time:
                        filtered_data.append(item)
                except:
                    continue
            
            # FaylÄ± yazar
            with open(self.seen_news_file, 'w', encoding='utf-8') as f:
                json.dump(filtered_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"{len(filtered_data)} xÉ™bÉ™r hash-i saxlanÄ±ldÄ±")
        except Exception as e:
            logger.error(f"GÃ¶rÃ¼lÉ™n xÉ™bÉ™rlÉ™r saxlama xÉ™tasÄ±: {e}")

    def _is_news_seen(self, news_item: NewsItem) -> bool:
        """XÉ™bÉ™rin É™vvÉ™lcÉ™dÉ™n gÃ¶rÃ¼ldÃ¼yÃ¼nÃ¼ yoxlayÄ±r"""
        return news_item.hash in self.seen_news

    def _mark_news_as_seen(self, news_item: NewsItem):
        """XÉ™bÉ™ri gÃ¶rÃ¼ldÃ¼ olaraq iÅŸarÉ™lÉ™yir"""
        self.seen_news.add(news_item.hash)
        self._save_seen_news(news_item)

    def fetch_coindesk_news(self) -> List[NewsItem]:
        try:
            news_items = []
            source_config = NEWS_SOURCES['coindesk']
            feed = feedparser.parse(source_config['rss_url'])
            for entry in feed.entries[:10]:
                try:
                    title = entry.title
                    url = entry.link
                    summary = entry.summary if hasattr(entry, 'summary') else ""
                    published = datetime(*entry.published_parsed[:6])
                    if published > datetime.now() - timedelta(days=1):
                        content = self._fetch_article_content(url)
                        news_item = NewsItem(
                            title=title,
                            content=content,
                            url=url,
                            source=source_config['name'],
                            published_date=published,
                            summary=summary
                        )
                        if not self._is_news_seen(news_item):
                            news_items.append(news_item)
                            self._mark_news_as_seen(news_item)
                except Exception as e:
                    logger.error(f"CoinDesk xÉ™bÉ™r emal xÉ™tasÄ±: {e}")
            return news_items
        except Exception as e:
            logger.error(f"CoinDesk RSS xÉ™tasÄ±: {e}")
            return []

    def fetch_theblock_news(self) -> List[NewsItem]:
        try:
            news_items = []
            source_config = NEWS_SOURCES['theblock']
            feed = feedparser.parse(source_config['rss_url'])
            for entry in feed.entries[:10]:
                try:
                    title = entry.title
                    url = entry.link
                    summary = entry.summary if hasattr(entry, 'summary') else ""
                    published = datetime(*entry.published_parsed[:6])
                    if published > datetime.now() - timedelta(days=1):
                        content = self._fetch_article_content(url)
                        news_item = NewsItem(
                            title=title,
                            content=content,
                            url=url,
                            source=source_config['name'],
                            published_date=published,
                            summary=summary
                        )
                        if not self._is_news_seen(news_item):
                            news_items.append(news_item)
                            self._mark_news_as_seen(news_item)
                except Exception as e:
                    logger.error(f"The Block xÉ™bÉ™r emal xÉ™tasÄ±: {e}")
            return news_items
        except Exception as e:
            logger.error(f"The Block RSS xÉ™tasÄ±: {e}")
            return []



    def _fetch_article_content(self, url: str) -> str:
        try:
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                html = response.text
                soup = BeautifulSoup(html, 'html.parser')
                paragraphs = soup.find_all(['p', 'div'], class_=lambda x: x and any(
                    keyword in x.lower() for keyword in ['content', 'article', 'body', 'text']
                ))
                if not paragraphs:
                    paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs[:5]])
                return content[:1000]
        except Exception as e:
            logger.error(f"MÉ™qalÉ™ mÉ™zmunu Ã§É™kmÉ™ xÉ™tasÄ±: {e}")
        return ""

    def fetch_cryptonews_news(self) -> List[NewsItem]:
        try:
            news_items = []
            source_config = NEWS_SOURCES['cryptonews']
            feed = feedparser.parse(source_config['rss_url'])
            for entry in feed.entries[:10]:
                try:
                    title = entry.title
                    url = entry.link
                    summary = entry.summary if hasattr(entry, 'summary') else ""
                    published = datetime(*entry.published_parsed[:6])
                    if published > datetime.now() - timedelta(days=1):
                        content = self._fetch_article_content(url)
                        news_item = NewsItem(
                            title=title,
                            content=content,
                            url=url,
                            source=source_config['name'],
                            published_date=published,
                            summary=summary
                        )
                        if not self._is_news_seen(news_item):
                            news_items.append(news_item)
                            self._mark_news_as_seen(news_item)
                except Exception as e:
                    logger.error(f"Crypto News xÉ™bÉ™r emal xÉ™tasÄ±: {e}")
            return news_items
        except Exception as e:
            logger.error(f"Crypto News RSS xÉ™tasÄ±: {e}")
            return []

    def fetch_newsbtc_news(self) -> List[NewsItem]:
        try:
            news_items = []
            source_config = NEWS_SOURCES['newsbtc']
            feed = feedparser.parse(source_config['rss_url'])
            for entry in feed.entries[:10]:
                try:
                    title = entry.title
                    url = entry.link
                    summary = entry.summary if hasattr(entry, 'summary') else ""
                    published = datetime(*entry.published_parsed[:6])
                    if published > datetime.now() - timedelta(days=1):
                        content = self._fetch_article_content(url)
                        news_item = NewsItem(
                            title=title,
                            content=content,
                            url=url,
                            source=source_config['name'],
                            published_date=published,
                            summary=summary
                        )
                        if not self._is_news_seen(news_item):
                            news_items.append(news_item)
                            self._mark_news_as_seen(news_item)
                except Exception as e:
                    logger.error(f"NewsBTC xÉ™bÉ™r emal xÉ™tasÄ±: {e}")
            return news_items
        except Exception as e:
            logger.error(f"NewsBTC RSS xÉ™tasÄ±: {e}")
            return []

    def fetch_all_news(self) -> List[NewsItem]:
        all_news = []
        try:
            results = [
                self.fetch_coindesk_news(),
                self.fetch_theblock_news(),
                self.fetch_cryptonews_news(),
                self.fetch_newsbtc_news()
            ]
            for result in results:
                if isinstance(result, list):
                    all_news.extend(result)
                elif isinstance(result, Exception):
                    logger.error(f"XÉ™bÉ™r Ã§É™kmÉ™ xÉ™tasÄ±: {result}")
            all_news.sort(key=lambda x: x.published_date, reverse=True)
            logger.info(f"CÉ™mi {len(all_news)} yeni xÉ™bÉ™r tapÄ±ldÄ±")
            return all_news
        except Exception as e:
            logger.error(f"Ãœmumi xÉ™bÉ™r Ã§É™kmÉ™ xÉ™tasÄ±: {e}")
            return []

    def cleanup_seen_news(self, hours: int = 24):
        """KÃ¶hnÉ™ xÉ™bÉ™rlÉ™ri tÉ™mizlÉ™yir"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        try:
            # Fayl mÉ™lumatlarÄ±nÄ± yenidÉ™n yÃ¼klÉ™yir vÉ™ kÃ¶hnÉ™lÉ™ri silir
            if os.path.exists(self.seen_news_file):
                with open(self.seen_news_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # YalnÄ±z son saatlar É™rzindÉ™ki xÉ™bÉ™rlÉ™ri saxlayÄ±r
                filtered_data = []
                for item in data:
                    saved_at = datetime.fromisoformat(item['saved_at'])
                    if saved_at > cutoff_time:
                        filtered_data.append(item)
                
                # YenilÉ™nmiÅŸ mÉ™lumatlarÄ± saxlayÄ±r
                with open(self.seen_news_file, 'w', encoding='utf-8') as f:
                    json.dump(filtered_data, f, ensure_ascii=False, indent=2)
                
                # Memory-dÉ™ki seti dÉ™ yenilÉ™yir
                self.seen_news = {item['hash'] for item in filtered_data}
                logger.info(f"Temizlik: {len(filtered_data)} xÉ™bÉ™r saxlandÄ±, kÃ¶hnÉ™lÉ™r silindi")
        except Exception as e:
            logger.error(f"Temizlik xÉ™tasÄ±: {e}")
    
    def emergency_reset_seen_news(self):
        """TÉ™cili vÉ™ziyyÉ™tdÉ™ bÃ¼tÃ¼n gÃ¶rÃ¼lÉ™n xÉ™bÉ™rlÉ™ri tÉ™mizlÉ™yir"""
        try:
            # Memory cache-i tÉ™mizlÉ™
            self.seen_news.clear()
            
            # FaylÄ± backup et vÉ™ sil
            if os.path.exists(self.seen_news_file):
                backup_name = f"{self.seen_news_file}.emergency_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                os.rename(self.seen_news_file, backup_name)
                logger.warning(f"ğŸš¨ EMERGENCY RESET: seen_news fayl backup edildi: {backup_name}")
            
            # Yeni boÅŸ fayl yarat
            with open(self.seen_news_file, 'w', encoding='utf-8') as f:
                json.dump([], f)
            
            logger.warning("ğŸš¨ EMERGENCY RESET: BÃ¼tÃ¼n gÃ¶rÃ¼lÉ™n xÉ™bÉ™r mÉ™lumatlarÄ± tÉ™mizlÉ™ndi!")
            return True
            
        except Exception as e:
            logger.error(f"Emergency reset xÉ™tasÄ±: {e}")
            return False

    def get_seen_news_stats(self) -> Dict:
        """GÃ¶rÃ¼lÉ™n xÉ™bÉ™rlÉ™r haqqÄ±nda statistika qaytarÄ±r"""
        try:
            stats = {
                'total_seen': len(self.seen_news),
                'recent_news': []
            }
            
            if os.path.exists(self.seen_news_file):
                with open(self.seen_news_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    stats['file_entries'] = len(data)
                    
                    # Son 5 xÉ™bÉ™ri gÃ¶stÉ™r
                    recent = sorted(data, key=lambda x: x.get('saved_at', ''), reverse=True)[:5]
                    for item in recent:
                        stats['recent_news'].append({
                            'title': item.get('title', 'N/A')[:50] + '...',
                            'source': item.get('source', 'N/A'),
                            'saved_at': item.get('saved_at', 'N/A')
                        })
            
            return stats
        except Exception as e:
            logger.error(f"Statistika alÄ±nma xÉ™tasÄ±: {e}")
            return {'total_seen': len(self.seen_news), 'error': str(e)} 